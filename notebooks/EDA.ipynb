{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE YOU RUN\n",
    "## Required Project Structure\n",
    "To ensure that all notebooks and scripts run correctly, please maintain the following structure at the **project root**. Note that the `data/` folder must be at the same level as the `notebooks/` and `src/` folders.\n",
    "\n",
    "```text\n",
    ".\n",
    "├── api/                                  # FastAPI/Flask code\n",
    "├── data/                                 # [IGNORED BY GIT - LOCAL ONLY]\n",
    "│   ├── cyber_dataset/\n",
    "│   │   ├── auth.txt.gz                   # Authentication logs (Large)\n",
    "│   │   └── redteam.txt.gz                # Redteam labels\n",
    "│   ├── elliptic_bitcoin_dataset/\n",
    "│   │   ├── elliptic_txs_classes.csv\n",
    "│   │   ├── elliptic_txs_edgelist.csv\n",
    "│   │   └── elliptic_txs_features.csv\n",
    "│   └── keystroke_dynamics_dataset/\n",
    "│       └── DSL-StrongPasswordData.csv\n",
    "├── notebooks/\n",
    "│   └── EDA.ipynb                         # Current Notebook\n",
    "├── src/                                  # Source code for training/API\n",
    "├── vis/                                  # Visualization assets\n",
    "├── .gitignore\n",
    "├── README.md\n",
    "└── requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the Data:\n",
    "- Elliptic is already a graph. It has 2% illicit data, the rest is licit. \n",
    "- Few things to check out: is the data clustered in specific time windows or not?\n",
    "\n",
    "- For device/auth signals, this data is pretty massive. We need to downsample to be able to manage number of users. \n",
    "- We can explore how many users log into a single machine. Maybe a \"normal\" number of devices per user, things like that\n",
    "\n",
    "- Behavioral data is micro-features for the nodes. We can analyze the variance in keystoke hold times. Maybe some sort of behavioral fingerprint for a user that changes when an account is compromised.\n",
    "\n",
    "Mapping the Data\n",
    "- These obviouslly aren't natively connected, we need to do this ourselves. For example, User_ID -> Wallet_ID and Device_ID -> Login_Session or something like that. \n",
    "- This could help us map User A logs in from Device B (unusual) with a typing cadence that doesn't match their profile, then immediately sends Bitcoin to an Illicit node in the Elliptic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Datasets\n",
    "df_labels = pd.read_csv('../data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv') # maps transaction IDs to licit, illicit, unknown\n",
    "df_edges = pd.read_csv('../data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv') # the adjacency list [who sent money to who]\n",
    "df_features = pd.read_csv('../data/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None) # 166 features for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rename columns for clarity\n",
    "# The features file usually doesn't have headers. Column 0 is the ID, Column 1 is Time Step.\n",
    "df_features.columns = ['txId', 'time_step'] + [f'feat_{i}' for i in range(165)]\n",
    "\n",
    "# 2. Merge labels and features into a master Node Dataframe\n",
    "df_nodes = pd.merge(df_features, df_labels, on='txId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {'1': 1, '2': 2, 'unknown': -1}\n",
    "df_nodes['class'] = df_nodes['class'].map(class_map)\n",
    "print(df_nodes['class'].value_counts())\n",
    "# We have a lot of unknown nodes we can work with. Train on the known ones and see if we can classify the unknowns later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal dist\n",
    "df_nodes.groupby('time_step')['class'].count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph density \n",
    "num_nodes = df_nodes.shape[0]\n",
    "num_edges = df_edges.shape[0]\n",
    "density = num_edges / (num_nodes * (num_nodes - 1))\n",
    "print(f\"Graph Density: {density:.6f}\")\n",
    "#sparse graph is excpected for real world networks, network is highly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_edges)/len(df_nodes)  # avg degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree for each node\n",
    "# In-degree: how many transactions flow INTO this tx\n",
    "# Out-degree: how many transactions flow OUT of this tx\n",
    "degrees = df_edges['txId1'].value_counts() + df_edges['txId2'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(degrees, bins=50, log=True, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Degree Distribution (Log Scale)\")\n",
    "plt.xlabel(\"Number of Connections (Degree)\")\n",
    "plt.ylabel(\"Count of Nodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting with some context here:\n",
    "- the average degree is 1.15ish, meaning that any node with a degree of 10, 20, or more, is likely a major outlier and this represents mixing services or consolidation points \n",
    "- Because of sparsity, we should use GraphSAGE or GCN because it aggregates neighbors information without being overwhelmed by noise. \n",
    "\n",
    "- Another thing, isolated clusters can be highly suspicious. There are a lot of disjointed clusters in this data. So, most nodes do not see each other. We need to rely on local neighborhood features because of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Graph object from your edgelist\n",
    "G = nx.from_pandas_edgelist(df_edges, source='txId1', target='txId2', create_using=nx.Graph())\n",
    "\n",
    "# 2. Find all connected components\n",
    "components = list(nx.connected_components(G))\n",
    "\n",
    "# 3. Sort components by size (largest first)\n",
    "components.sort(key=len, reverse=True)\n",
    "\n",
    "# 4. Get the Largest Connected Component\n",
    "lcc = G.subgraph(components[0])\n",
    "\n",
    "print(f\"Total nodes in full graph: {G.number_of_nodes()}\")\n",
    "print(f\"Nodes in the Largest Connected Component: {lcc.number_of_nodes()}\")\n",
    "print(f\"Percentage of nodes in LCC: {100 * lcc.number_of_nodes() / G.number_of_nodes():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of txId to class for quick lookup\n",
    "node_class_dict = dict(zip(df_nodes['txId'], df_nodes['class']))\n",
    "\n",
    "# Analyze the top 10 largest components\n",
    "for i, comp in enumerate(components[:10]):\n",
    "    # Count classes in this component\n",
    "    classes_in_comp = [node_class_dict.get(node, -1) for node in comp]\n",
    "    \n",
    "    fraud_count = classes_in_comp.count(1)\n",
    "    licit_count = classes_in_comp.count(0)\n",
    "    unknown_count = classes_in_comp.count(-1)\n",
    "    \n",
    "    print(f\"Component {i+1} | Size: {len(comp)} | Fraud: {fraud_count} | Licit: {licit_count} | Unknown: {unknown_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use this to create risk scores for unknown nodes based on their proximity to known fraudulent nodes in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT DATA\n",
    "KEYSTROKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_bio = pd.read_csv('data/keystroke_dynamics_dataset/DSL-StrongPasswordData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Stats: How many users and how many samples per user?\n",
    "num_users = df_bio['subject'].nunique()\n",
    "samples_per_user = df_bio.groupby('subject').size().iloc[0]\n",
    "print(f\"Total Users: {num_users}\")\n",
    "print(f\"Samples per User: {samples_per_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few users to compare\n",
    "sample_users = df_bio['subject'].unique()[:5]\n",
    "df_sample = df_bio[df_bio['subject'].isin(sample_users)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='subject', y='H.period', data=df_sample)\n",
    "plt.title('Key \"H\" Hold Time Distribution across 5 Users')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try some synethic cross-channel linking of this data artficially. This could look like s002 (keystrokes) -> txID_12345 (elliptic). Some issues with this...\n",
    "- There are only 51 users, and about 200k transactions, so the data would be very repetitive.\n",
    "- We could limit where we merge (active components, ignoring unknown nodes, etc) and only work with that small subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next dataset is harder to work with due to file sizes:\n",
    "- auth.txt.gz, we NEED this one. It has auth events and is the device/access layer \n",
    "- redteam.txt.gz, pretty much need this. Contains confirmed malicious activity. can see timestamps/users to simulate the start of a fraud event in the graph\n",
    "- proc.txt.gz, probably DONT need this. It has start/stop events and can detect bot behavior, maybe a nice to have later\n",
    "- dnd.txt.gz, probably DONT need this. DND lookup events, useful for network security, but harder to map to financial fraud. can ignore\n",
    "\n",
    "We can join with src_user and src_comp to make a heterogeneous graph where edges are \"Sent Money To\" \"Logged Into\" or \"Typed\"\n",
    "\n",
    "I'm waiting for the data here to load and will sample and upload a sample of that data for us to use for the purpose of the project before we expand how much data we are grabbing from the auth file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
